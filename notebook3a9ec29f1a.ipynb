{"metadata":{"kernelspec":{"display_name":"machine_learning","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9013505,"sourceType":"datasetVersion","datasetId":5430856}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#FAF3F3; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #E57373; margin-top: 20px;\">\n    <h1 style=\"font-size:24px; font-family:Georgia, serif; color:#D32F2F; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        Credit Card Fraud Detection: Model Training and Analysis\n    </h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#E6F9E6; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 20px;\">\n    <h1 style=\"font-size:24px; font-family:Georgia, serif; color:#4A90E2; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        About Author\n    </h1>\n    <div style=\"text-align:center;\">\n        <a href=\"https://www.sak.kesug.com\">\n            <img src=\"https://media.licdn.com/dms/image/v2/D4D03AQEsEJ_gVNnU3w/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1720678393429?e=2147483647&v=beta&t=RALfVAKvT6TmP2BBpil_CGzZK5L5ykNUou5yModXTVw\" style=\"width: 20%; border-radius: 50%;\" alt=\"Shaheer Ali\">\n        </a>\n    </div>\n    <h2 style=\"font-size:20px; font-family:Georgia, serif; color:#333; text-align:center;\">Mr. Shaheer Ali</h2>\n    <p style=\"font-size:16px; font-family:Georgia, serif; line-height: 1.5em; text-align:center; color:#333;\">\n        <em>Data Scientist / ML Engineer</em><br>\n        BS Computer Science\n    </p>\n    <div style=\"text-align:center; margin-top: 10px;\">\n        <a href=\"https://www.youtube.com/channel/UCUTphw52izMNv9W6AOIFGJA\">\n            <img width=\"50\" height=\"50\" src=\"https://img.icons8.com/color/48/youtube-play.png\" alt=\"youtube-play\">\n        </a>\n        <a href=\"https://twitter.com/__shaheerali190\">\n            <img width=\"50\" height=\"50\" src=\"https://img.icons8.com/dotty/80/x.png\" alt=\"x\">\n        </a>\n        <a href=\"https://www.linkedin.com/in/shaheer-ali-2761aa303/\">\n            <img width=\"50\" height=\"50\" src=\"https://img.icons8.com/color/48/linkedin.png\" alt=\"linkedin\">\n        </a>\n        <a href=\"https://github.com/shaheeralics\">\n            <img width=\"50\" height=\"50\" src=\"https://img.icons8.com/ios-glyphs/50/github.png\" alt=\"github\">\n        </a>\n        <a href=\"https://www.kaggle.com/shaheerali197\">\n            <img width=\"50\" height=\"50\" src=\"https://img.icons8.com/clouds/100/kaggle.png\" alt=\"kaggle\">\n        </a>\n    </div>\n    <div style=\"text-align:center; margin-top: 20px;\">\n        <a href=\"https://www.sak.kesug.com\" style=\"font-size:18px; font-family:Georgia, serif; color:#4A90E2; text-decoration:none; border:2px solid #4A90E2; padding: 5px 15px; border-radius: 5px;\">\n            Portfolio Site\n        </a>\n    </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#E6F9E6; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 20px;\">\n    <h1 style=\"font-size:24px; font-family:Georgia, serif; color:#4A90E2; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        About the Dataset\n    </h1>\n    <p style=\"font-size:16px; font-family:Georgia, serif; line-height: 1.5em; text-align:center; color:#333;\">\n        The Credit Card Transactions Dataset provides detailed records of credit card transactions, including information about transaction times, amounts, and associated personal and merchant details. This dataset has over 1.85M rows.\n    </p>\n    <h2 style=\"font-size:20px; font-family:Georgia, serif; color:#333; text-align:center;\">How This Dataset Can Be Used:</h2>\n    <ul style=\"font-size:16px; font-family:Georgia, serif; line-height: 1.5em; color:#333; text-align:center;\">\n        <li><strong>Fraud Detection:</strong> Use machine learning models to identify fraudulent transactions by examining patterns in transaction amounts, locations, and user profiles. Enhancing fraud detection systems becomes feasible by analyzing behavioral patterns.</li>\n        <li><strong>Customer Segmentation:</strong> Segment customers based on spending patterns, location, and demographics. Tailor marketing strategies and personalized offers to these different customer segments for better engagement.</li>\n        <li><strong>Transaction Classification:</strong> Classify transactions into categories such as grocery or entertainment to understand spending behaviors. This helps in improving recommendation systems by identifying transaction categories and preferences.</li>\n        <li><strong>Geospatial Analysis:</strong> Analyze transaction data geographically to map spending patterns and detect regional trends or anomalies based on latitude and longitude.</li>\n        <li><strong>Predictive Modeling:</strong> Build models to forecast future spending behavior using historical transaction data. Predict potential fraudulent activities and financial trends.</li>\n        <li><strong>Behavioral Analysis:</strong> Examine how factors like transaction amount, merchant type, and time influence spending behavior. Study the relationships between user demographics and transaction patterns.</li>\n        <li><strong>Anomaly Detection:</strong> Identify unusual transaction patterns that deviate from normal behavior to detect potential fraud early. Employ anomaly detection techniques to spot outliers and suspicious activities.</li>\n    </ul>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#D0E6F3; padding: 12px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 12px;\">\n    <h1 style=\"font-size:28px; font-family:Georgia, serif; color:#000000; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        Importing Libraries\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# importing Classification models from sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# importing necessary libraries for model evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#D0E6F3; padding: 12px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 12px;\">\n    <h1 style=\"font-size:28px; font-family:Georgia, serif; color:#000000; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        Loading Dataset\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"#importing dataset which is avail in 6 different files\ndf = pd.DataFrame()\nfor i in range(1, 6):\n    df = pd.concat([df, pd.read_csv(f\"../../datasets/Projects/Credit Card/credit_card_transactions{i}.csv\")], axis=0)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#D0E6F3; padding: 12px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 12px;\">\n    <h1 style=\"font-size:28px; font-family:Georgia, serif; color:#000000; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        Exploratory Data Analysis\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# showing max clolumns\npd.set_option('display.max_columns', None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mid of the df like df.tail() etc\ndf.sample(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing the unwanted column 'Unnamed: 0'\ndf.drop('Unnamed: 0', axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# counting the number of object and int64 data types in dataset\ndf.dtypes.value_counts()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the missing values\ndf.isnull().sum()/len(df)*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As merch_zipcode can not be replaced with mean median mode, so we will drop the missing values","metadata":{}},{"cell_type":"code","source":"# removing the missing values of merch_zipcode\ndf = df.dropna(subset=['merch_zipcode'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the missing values again\ndf.isnull().sum()/len(df)*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the shape of dataset after removing the missing values","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for Duplicates","metadata":{}},{"cell_type":"code","source":"# Checking for Duplicates\ndf.duplicated().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It means there are no duplicates in the Dataset","metadata":{}},{"cell_type":"code","source":"# df.hist(figsize=(20, 20))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#D0E6F3; padding: 12px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 12px;\">\n    <h1 style=\"font-size:28px; font-family:Georgia, serif; color:#000000; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        Feature Engineering\n    </h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### 1.0 Feature extraction","metadata":{}},{"cell_type":"markdown","source":"DATE COLUMN","metadata":{}},{"cell_type":"code","source":"# first converting the date column to datetime format\ndf['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting the year, month, day, hour, and minute from the trans_date_trans_time column\ndf['Year'] = df['trans_date_trans_time'].dt.year\ndf['month'] = df['trans_date_trans_time'].dt.month\ndf['day'] = df['trans_date_trans_time'].dt.day\n\n# now dromping the date column\ndf = df.drop('trans_date_trans_time', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DOB COLUMN","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datetime import datetime\n\n# Calculate age from DOB\ndf['age'] = (datetime.now() - pd.to_datetime(df['dob'], errors='coerce')).dt.days // 365\n\n# Define bins and labels\nbins = [0, 18, 25, 35, 45, 55, 65, 101]\nlabels = [\n    'Young Children',              # 0 to 17\n    'Young Adult',        # 18 to 24\n    'Adult',              # 25 to 34\n    'Middle-Aged',        # 35 to 44\n    'Older Adult',        # 45 to 54\n    'Senior',             # 55 to 64\n    'Elderly'             # 65 and above\n]\n\n# Apply pd.cut to bin the age\ndf['age'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.age.min()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.age.max()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.age.value_counts().plot(kind='bar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.age.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now droping the date of birth column","metadata":{}},{"cell_type":"code","source":"df.drop('dob', axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.0 Feature Selection:","metadata":{}},{"cell_type":"code","source":"df.columns.nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So I have now to do the feature selection for the credit card fraud detection dataset, as there are `25` columns here in the dataset, i will apply `feature selection` for both the categorical columns and the numeric columns and then select 5 from categorical columns and top 5 from numeric columns.","metadata":{}},{"cell_type":"code","source":"df.dtypes.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will make 2 dataframes one for categories and other for numeric data, to make feature selection easy","metadata":{}},{"cell_type":"code","source":"# making a dataset which include only object data types with is_fraud column included in it\ncat = df.select_dtypes(include=['object', 'category'])\ncat['is_fraud'] = df['is_fraud']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = df.select_dtypes(exclude=['object', 'category'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before proceding forward, we have to Normalize the Data","metadata":{}},{"cell_type":"code","source":"num.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Assuming 'cat' is your DataFrame\nX = cat.drop('is_fraud', axis=1)\ny = cat['is_fraud']\n\n# Apply Label Encoding to categorical columns\nle = LabelEncoder()\nfor col in X.select_dtypes(include=['object','category']).columns:\n    X[col] = le.fit_transform(X[col])\n\n# Now X contains the encoded categorical features\n# Apply SelectKBest using the Chi-Square test\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X, y)\n\n# Making a DataFrame of the scores\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n# Concatenate the scores and column names for readability\nfeature_scores = pd.concat([dfcolumns, dfscores], axis=1)\nfeature_scores.columns = ['Feature', 'Score']  # Naming the columns\nprint(feature_scores.nlargest(10, 'Score'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 5 features to select","metadata":{}},{"cell_type":"code","source":"top_features = feature_scores.nlargest(8, 'Score')['Feature'].values\ntop_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I will filter out the numeric columns for finding the top 5 best features for training our model.","metadata":{}},{"cell_type":"code","source":"# now using the feature selection for numerical data types\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nX = num.drop('is_fraud', axis=1)\ny = num['is_fraud']\n\n# Apply SelectKBest using the ANOVA F-value test\nbestfeatures = SelectKBest(score_func=f_classif, k='all')  # SelectKBest will select the best 'k' features\nfit = bestfeatures.fit(X, y)\n\n# Making a DataFrame of the scores\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n# Concatenate the scores and column names for readability\nfeature_scores = pd.concat([dfcolumns, dfscores], axis=1)\nfeature_scores.columns = ['Feature', 'Score']  # Naming the columns\nprint(feature_scores.nlargest(10, 'Score'))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_features = feature_scores.nlargest(5, 'Score')['Feature'].values\ntop_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I will try an alternative method for the selection of best numeric columns for my model training.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Calculate the correlation of each feature with the target variable\ncorrelation = num.corr()\n\n# Extract correlations with the target column\ntarget_correlation = correlation['is_fraud'].drop('is_fraud')\n\n# Get the top 5 features with the highest absolute correlation values\ntop_features = target_correlation.abs().sort_values(ascending=False).head(8).index\n\n# Display the results\nprint(\"Top 8 features based on correlation with the target variable:\")\nprint(num[top_features].head())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using the anova test and the correlation , it is clear that the following models the top 5 numeric features for model training\n`'amt', 'month', 'Year', 'day', 'unix_time'`","metadata":{}},{"cell_type":"code","source":"# now making a data frame have the following features 'amt', 'month', 'Year', 'day', 'unix_time','trans_num', 'category', 'first', 'street', 'merchant' from df\ndf = df[['amt', 'month', 'Year', 'day', 'unix_time', 'merch_zipcode', 'merch_long', 'long','trans_num', 'category', 'first', 'street', 'merchant', 'last',\n       'gender', 'job','is_fraud']]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:#D0E6F3; padding: 12px; border-radius: 10px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1); border:2px solid #66C2A5; margin-top: 12px;\">\n    <h1 style=\"font-size:28px; font-family:Georgia, serif; color:#000000; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\">\n        Training Model Though Pipeline\n    </h1>\n</div>","metadata":{}},{"cell_type":"code","source":"df.nunique()<13","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('is_fraud', axis=1)\ny = df['is_fraud']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying label encoding on features greater than 14 unique values and one hot encoding on features less than 14 unique values, but \n# if both then apply label and one both on their respective requirements i.e label encoding on features greater than 14 unique values and one hot encoding on features less than 14 unique values\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Apply Label Encoding to categorical columns\nle = LabelEncoder()\nfor col in X.select_dtypes(include=['object','category']).columns:\n    if X[col].nunique() > 14:\n        X[col] = le.fit_transform(X[col])\n    else:\n        # Apply One-Hot Encoding to categorical columns with less than 14 unique values but the other features will not be removed\n        X = pd.get_dummies(X, columns=[col], drop_first=True)\n        \nX.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pipeline with preprocessor and RandomForestClassifier\npipeline = Pipeline(steps=[\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = pipeline.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now i have to check the model performance using confusion matrix\n# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n# plot the confusion matrix\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}